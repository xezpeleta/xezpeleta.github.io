<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Basque from speech to text: improving transcription models | Xabi Ezpeleta</title>
<meta name=keywords content="ai,basque,stt,whisper"><meta name=description content="Since 2022, I&rsquo;ve been training the original Whisper STT (speech-to-text) model using the Mozilla Common Voice dataset to improve its performance with the Basque language (through fine-tuning). STT models convert speech to text using natural language processing.
Compared to the original models, there was a significant improvement in results. As the Mozilla Common Voice initiative grew, the model performed much better.

But at the same time, it became clear that more voice data was needed to achieve a minimum quality level."><meta name=author content><link rel=canonical href=https://xezpeleta.github.io/en/blog/whisper-stt-hobekuntza/><link crossorigin=anonymous href=/assets/css/stylesheet.45e028aa8ce0961349adf411b013ee39406be2c0bc80d4ea3fc04555f7f4611a.css integrity="sha256-ReAoqozglhNJrfQRsBPuOUBr4sC8gNTqP8BFVff0YRo=" rel="preload stylesheet" as=style><link rel=icon href=https://xezpeleta.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://xezpeleta.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://xezpeleta.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://xezpeleta.github.io/apple-touch-icon.png><link rel=mask-icon href=https://xezpeleta.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=eu href=https://xezpeleta.github.io/blog/whisper-stt-hobekuntza/><link rel=alternate hreflang=en href=https://xezpeleta.github.io/en/blog/whisper-stt-hobekuntza/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-ZJRN8P7X6R"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-ZJRN8P7X6R")}</script><meta property="og:url" content="https://xezpeleta.github.io/en/blog/whisper-stt-hobekuntza/"><meta property="og:site_name" content="Xabi Ezpeleta"><meta property="og:title" content="Basque from speech to text: improving transcription models"><meta property="og:description" content="Since 2022, I’ve been training the original Whisper STT (speech-to-text) model using the Mozilla Common Voice dataset to improve its performance with the Basque language (through fine-tuning). STT models convert speech to text using natural language processing.
Compared to the original models, there was a significant improvement in results. As the Mozilla Common Voice initiative grew, the model performed much better.
But at the same time, it became clear that more voice data was needed to achieve a minimum quality level."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="blog"><meta property="article:published_time" content="2024-03-20T00:00:00+00:00"><meta property="article:modified_time" content="2024-03-20T00:00:00+00:00"><meta property="article:tag" content="Ai"><meta property="article:tag" content="Basque"><meta property="article:tag" content="Stt"><meta property="article:tag" content="Whisper"><meta property="og:image" content="https://xezpeleta.github.io/images/stt.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://xezpeleta.github.io/images/stt.png"><meta name=twitter:title content="Basque from speech to text: improving transcription models"><meta name=twitter:description content="Since 2022, I&rsquo;ve been training the original Whisper STT (speech-to-text) model using the Mozilla Common Voice dataset to improve its performance with the Basque language (through fine-tuning). STT models convert speech to text using natural language processing.
Compared to the original models, there was a significant improvement in results. As the Mozilla Common Voice initiative grew, the model performed much better.

But at the same time, it became clear that more voice data was needed to achieve a minimum quality level."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://xezpeleta.github.io/en/blog/"},{"@type":"ListItem","position":2,"name":"Basque from speech to text: improving transcription models","item":"https://xezpeleta.github.io/en/blog/whisper-stt-hobekuntza/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Basque from speech to text: improving transcription models","name":"Basque from speech to text: improving transcription models","description":"Since 2022, I\u0026rsquo;ve been training the original Whisper STT (speech-to-text) model using the Mozilla Common Voice dataset to improve its performance with the Basque language (through fine-tuning). STT models convert speech to text using natural language processing.\nCompared to the original models, there was a significant improvement in results. As the Mozilla Common Voice initiative grew, the model performed much better.\nBut at the same time, it became clear that more voice data was needed to achieve a minimum quality level.\n","keywords":["ai","basque","stt","whisper"],"articleBody":"Since 2022, I’ve been training the original Whisper STT (speech-to-text) model using the Mozilla Common Voice dataset to improve its performance with the Basque language (through fine-tuning). STT models convert speech to text using natural language processing.\nCompared to the original models, there was a significant improvement in results. As the Mozilla Common Voice initiative grew, the model performed much better.\nBut at the same time, it became clear that more voice data was needed to achieve a minimum quality level.\nA few weeks ago, I found a new study from the HiTZ-Aholab research center, where they created a bilingual Nvidia NeMo model using various datasets: Mozilla Common Voice, OpenSLR, and the Basque Parliament corpus.\nInspired by this work, I decided to use the same data collection to improve the Whisper model. The results have improved significantly; for example, the WER (Word Error Rate) of the whisper-small-eu model has decreased from 11.84% to 7.63%.\nDatasets Based on the HiTZ-Aholab report, I used the following data collections:\nMozilla Common Voice OpenSLR Basque Parliament The combination of these datasets provides greater diversity, both in terms of speakers, recording quality, and context and topics.\nAdditionally, researcher Asier Herranz has made these data ready to use in an easy way: asierhv/composite_corpus_eu_v2.1. In total, we have 675.98 hours of training data available.\nResults Results of the new models:\nwhisper-large-v3-eu model updated, WER: 4.84 (previously: 7.21). whisper-medium-eu model updated, WER: 7.14 (previously: 8.80) whisper-small-eu model updated, WER: 7.63 (previously: 11.83) whisper-base-eu model (new), WER: 10.78 whisper-tiny-eu model (new), WER: 13.56 Note: The lower the WER value, the better.\nThe Basque Whisper models created are available here\nThese evaluations were performed using the test portion of the Mozilla Common Voice 18.0 dataset.\nSimilar Alternatives In addition to the research mentioned earlier, this other work published by Vicomtech is also interesting. Similarly, and using the same dataset, they trained a Basque-Spanish Whisper model and achieved excellent results. Unfortunately, these models have not been made available (as far as I know). HiTZ/stt_eu_conformer_transducer_large Basque Speech-to-Text model Conformer-Transducer. Based on the Nvidia NeMo model and with very good results: 2.79% WER. Elhuyar Aditu is also a speech recognizer that performs automatic transcriptions. It is not free software and is paid, but they offer the option to try it from their website. ","wordCount":"380","inLanguage":"en","image":"https://xezpeleta.github.io/images/stt.png","datePublished":"2024-03-20T00:00:00Z","dateModified":"2024-03-20T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://xezpeleta.github.io/en/blog/whisper-stt-hobekuntza/"},"publisher":{"@type":"Organization","name":"Xabi Ezpeleta","logo":{"@type":"ImageObject","url":"https://xezpeleta.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://xezpeleta.github.io/en/ accesskey=h title="Xabi Ezpeleta (Alt + H)">Xabi Ezpeleta</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://xezpeleta.github.io/ title=Euskara aria-label=Euskara>Eu</a></li></ul></div></div><ul id=menu><li><a href=https://xezpeleta.github.io/en/categories/ title=categories><span>categories</span></a></li><li><a href=https://xezpeleta.github.io/en/tags/ title=tags><span>tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Basque from speech to text: improving transcription models</h1><div class=post-meta><span title='2024-03-20 00:00:00 +0000 UTC'>March 20, 2024</span>&nbsp;|&nbsp;Translations:<ul class=i18n_list><li><a href=https://xezpeleta.github.io/blog/whisper-stt-hobekuntza/>Eu</a></li></ul></div></header><figure class=entry-cover><img loading=eager src=https://xezpeleta.github.io/images/stt.png alt=Speech-to-text><p>Basque from speech to text</p></figure><div class=post-content><p>Since 2022, I&rsquo;ve been training the original <a href=https://openai.com/index/whisper/>Whisper</a> STT (<em>speech-to-text</em>) model using the <a href=https://commonvoice.mozilla.org/>Mozilla Common Voice</a> dataset to improve its performance with the Basque language (through <a href=https://en.wikipedia.org/wiki/Fine-tuning_%28deep_learning%29><em>fine-tuning</em></a>). <em>STT</em> models convert speech to text using natural language processing.</p><p>Compared to the original models, there was a significant improvement in results. As the Mozilla Common Voice initiative grew, the model performed much better.</p><blockquote><p>But at the same time, it became clear that more voice data was needed to achieve a minimum quality level.</p></blockquote><p>A few weeks ago, I found <a href=https://www.isca-archive.org/iberspeech_2024/herranz24_iberspeech.pdf>a new study from the HiTZ-Aholab research center</a>, where they created a bilingual <em>Nvidia NeMo</em> model using various datasets: Mozilla Common Voice, OpenSLR, and the Basque Parliament corpus.</p><p>Inspired by this work, I decided to use the same data collection to improve the Whisper model. The results have improved significantly; for example, the <strong>WER (Word Error Rate) of the <code>whisper-small-eu</code> model has decreased from 11.84% to 7.63%</strong>.</p><h2 id=datasets>Datasets<a hidden class=anchor aria-hidden=true href=#datasets>#</a></h2><p>Based on the HiTZ-Aholab report, I used the following data collections:</p><ul><li>Mozilla Common Voice</li><li>OpenSLR</li><li>Basque Parliament</li></ul><p>The combination of these datasets provides greater diversity, both in terms of speakers, recording quality, and context and topics.</p><p>Additionally, researcher Asier Herranz has made these data ready to use in an easy way: <a href=https://huggingface.co/datasets/asierhv/composite_corpus_eu_v2.1>asierhv/composite_corpus_eu_v2.1</a>. In total, we have <strong>675.98 hours of training data</strong> available.</p><h2 id=results>Results<a hidden class=anchor aria-hidden=true href=#results>#</a></h2><p>Results of the new models:</p><ul><li><code>whisper-large-v3-eu</code> model updated, <strong>WER: 4.84</strong> (previously: 7.21).</li><li><code>whisper-medium-eu</code> model updated, <strong>WER: 7.14</strong> (previously: 8.80)</li><li><code>whisper-small-eu</code> model updated, <strong>WER: 7.63</strong> (previously: 11.83)</li><li><code>whisper-base-eu</code> model (new), <strong>WER: 10.78</strong></li><li><code>whisper-tiny-eu</code> model (new), <strong>WER: 13.56</strong></li></ul><p><em>Note: The lower the WER value, the better.</em></p><p>The Basque <strong>Whisper</strong> models created are <a href=https://huggingface.co/collections/xezpeleta/whisper-basque-fine-tuning-67b05797b023991df1715a51>available here</a></p><p>These evaluations were performed using the <code>test</code> portion of the <code>Mozilla Common Voice 18.0</code> dataset.</p><h2 id=similar-alternatives>Similar Alternatives<a hidden class=anchor aria-hidden=true href=#similar-alternatives>#</a></h2><ul><li>In addition to the research mentioned earlier, <a href=https://www.isca-archive.org/iberspeech_2024/vasquezcorrea24_iberspeech.pdf>this other work published by Vicomtech</a> is also interesting. Similarly, and using the same dataset, they trained a Basque-Spanish <em>Whisper</em> model and achieved <strong>excellent results</strong>. Unfortunately, these models <strong>have not been made available</strong> (as far as I know).</li><li><a href=https://huggingface.co/HiTZ/stt_eu_conformer_transducer_large>HiTZ/stt_eu_conformer_transducer_large</a> Basque Speech-to-Text model Conformer-Transducer. Based on the Nvidia NeMo model and with <strong>very good results</strong>: 2.79% WER.</li><li><a href=https://aditu.eus/>Elhuyar Aditu</a> is also a <em>speech recognizer</em> that performs automatic transcriptions. It is not free software and is paid, but they offer the option to try it from their website.</li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://xezpeleta.github.io/en/tags/ai/>Ai</a></li><li><a href=https://xezpeleta.github.io/en/tags/basque/>Basque</a></li><li><a href=https://xezpeleta.github.io/en/tags/stt/>Stt</a></li><li><a href=https://xezpeleta.github.io/en/tags/whisper/>Whisper</a></li></ul></footer></article></main><footer class=footer><span>Copyright © 2021, Xabier Ezpeleta. License <a href=https://creativecommons.org/licenses/by-sa/4.0/>CC BY-SA 4.0</a>.</span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>