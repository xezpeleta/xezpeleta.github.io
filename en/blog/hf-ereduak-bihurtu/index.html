<!doctype html><html lang=en dir=auto data-theme=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Convert Models to GGUF Format | Xabi Ezpeleta</title><meta name=keywords content="llm,llama.cpp,gguf,qwen"><meta name=description content="This guide explains the steps to convert models published on HuggingFace to GGUF format.
The GGUF format offers the possibility to run models efficiently on our computers. For this, we can use tools like Llama.cpp or Ollama.
Download Llama.cpp
To convert to GGUF format locally on our machine, we first need to download llama.cpp to be able to use its conversion tools.
# Clone the llama.cpp repository
git clone https://github.com/ggml-org/llama.cpp.git
Prepare the environment
We will use uv to prepare the environment and install necessary dependencies. If you haven&rsquo;t installed it, you will need to install it."><meta name=author content><link rel=canonical href=https://xezpeleta.github.io/en/blog/hf-ereduak-bihurtu/><link crossorigin=anonymous href=/assets/css/stylesheet.da3211e5ef867bf2b75fd5a6515cfed7195c011e8ab735694e203810a827097b.css integrity="sha256-2jIR5e+Ge/K3X9WmUVz+1xlcAR6KtzVpTiA4EKgnCXs=" rel="preload stylesheet" as=style><link rel=icon href=https://xezpeleta.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://xezpeleta.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://xezpeleta.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://xezpeleta.github.io/apple-touch-icon.png><link rel=mask-icon href=https://xezpeleta.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=eu href=https://xezpeleta.github.io/blog/hf-ereduak-bihurtu/><link rel=alternate hreflang=en href=https://xezpeleta.github.io/en/blog/hf-ereduak-bihurtu/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-ZJRN8P7X6R"></script><script>var doNotTrack=!1,dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes";if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-ZJRN8P7X6R")}</script><meta property="og:url" content="https://xezpeleta.github.io/en/blog/hf-ereduak-bihurtu/"><meta property="og:site_name" content="Xabi Ezpeleta"><meta property="og:title" content="Convert Models to GGUF Format"><meta property="og:description" content="This guide explains the steps to convert models published on HuggingFace to GGUF format.
The GGUF format offers the possibility to run models efficiently on our computers. For this, we can use tools like Llama.cpp or Ollama.
Download Llama.cpp To convert to GGUF format locally on our machine, we first need to download llama.cpp to be able to use its conversion tools.
# Clone the llama.cpp repository git clone https://github.com/ggml-org/llama.cpp.git Prepare the environment We will use uv to prepare the environment and install necessary dependencies. If you haven’t installed it, you will need to install it."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="blog"><meta property="article:published_time" content="2026-02-08T00:00:00+00:00"><meta property="article:modified_time" content="2026-02-08T00:00:00+00:00"><meta property="article:tag" content="Llm"><meta property="article:tag" content="Llama.cpp"><meta property="article:tag" content="Gguf"><meta property="article:tag" content="Qwen"><meta property="og:image" content="https://xezpeleta.github.io/images/hf-ereduak-bihurtu-cover.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://xezpeleta.github.io/images/hf-ereduak-bihurtu-cover.png"><meta name=twitter:title content="Convert Models to GGUF Format"><meta name=twitter:description content="This guide explains the steps to convert models published on HuggingFace to GGUF format.
The GGUF format offers the possibility to run models efficiently on our computers. For this, we can use tools like Llama.cpp or Ollama.
Download Llama.cpp
To convert to GGUF format locally on our machine, we first need to download llama.cpp to be able to use its conversion tools.
# Clone the llama.cpp repository
git clone https://github.com/ggml-org/llama.cpp.git
Prepare the environment
We will use uv to prepare the environment and install necessary dependencies. If you haven&rsquo;t installed it, you will need to install it."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://xezpeleta.github.io/en/blog/"},{"@type":"ListItem","position":2,"name":"Convert Models to GGUF Format","item":"https://xezpeleta.github.io/en/blog/hf-ereduak-bihurtu/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Convert Models to GGUF Format","name":"Convert Models to GGUF Format","description":"This guide explains the steps to convert models published on HuggingFace to GGUF format.\nThe GGUF format offers the possibility to run models efficiently on our computers. For this, we can use tools like Llama.cpp or Ollama.\nDownload Llama.cpp To convert to GGUF format locally on our machine, we first need to download llama.cpp to be able to use its conversion tools.\n# Clone the llama.cpp repository git clone https://github.com/ggml-org/llama.cpp.git Prepare the environment We will use uv to prepare the environment and install necessary dependencies. If you haven\u0026rsquo;t installed it, you will need to install it.\n","keywords":["llm","llama.cpp","gguf","qwen"],"articleBody":"This guide explains the steps to convert models published on HuggingFace to GGUF format.\nThe GGUF format offers the possibility to run models efficiently on our computers. For this, we can use tools like Llama.cpp or Ollama.\nDownload Llama.cpp To convert to GGUF format locally on our machine, we first need to download llama.cpp to be able to use its conversion tools.\n# Clone the llama.cpp repository git clone https://github.com/ggml-org/llama.cpp.git Prepare the environment We will use uv to prepare the environment and install necessary dependencies. If you haven’t installed it, you will need to install it.\n# Create a virtual environment cd llama.cpp uv venv # If you encounter issues, try specifying the Python version # uv venv --python 3.11 # Activate the environment source .venv/bin/activate Install dependencies In this case, we don’t need to install the full llama.cpp to convert models; we just need the libraries to use the conversion tools:\nuv pip install -r requirements/requirements-convert_hf_to_gguf.txt Convert to GGUF format By passing the identifier of the model located on HuggingFace (/) and specifying the desired output type, it will create our gguf file:\nIn this case, we will convert the 4B version of the HiTZ/Latxa-Qwen3-VL-4B-Instruct model, with f16 and q8_0 quantizations.\n# F16 uv run convert_hf_to_gguf.py HiTZ/Latxa-Qwen3-VL-4B-Instruct --remote --outtype f16 # Q8_0 uv run convert_hf_to_gguf.py HiTZ/Latxa-Qwen3-VL-4B-Instruct --remote --outtype q8_0 These commands will create the gguf files (e.g. HiTZ-Latxa-Qwen3-VL-4B-Instruct-q8_0.gguf).\nmmproj image processor for multimodal models (VL) To be able to pass images to VL type multimodal models (e.g. Latxa VL models), we will also need the image encoder. They are usually files named mmproj, which are often shared along with the model weights. We can download and use the original one (in this case, Qwen model’s one).\nHowever, we can also create this file ourselves, using the same conversion script and passing the --mmproj parameter.\n# Create mmproj file by adding `--mmproj` uv run convert_hf_to_gguf.py HiTZ/Latxa-Qwen3-VL-2B-Instruct --remote --outfile mmproj-HiTZ-Latxa-Qwen3-VL-2B-Instruct-q8_0.gguf --outtype q8_0 --mmproj Obtaining other quantizations If you want to obtain other quantizations, you will need to follow the Llama.cpp installation steps and use the llama-quantize command.\n# Quantize as Q4_K_M ./llama-quantize ggml-model-f16.gguf ggml-model-Q4_K_M.gguf Q4_K_M For more information, read the llama-quantize guide\nUsage with llama.cpp Once our gguf files are created, we can run the models locally on our computer using llama.cpp.\nFor this, yes, we will need to install llama.cpp.\n# Run llama-cli with our model llama-cli -m Orai-Kimu-9B-GGUF-Q4_0.gguf This command will allow us to do quick tests right in the terminal.\nI prefer, however, to run a server compatible with the OpenAI API, complete with a web interface!\nllama-server \\ --host localhost \\ --port 8000 \\ --model Gemma-2b-GGUF-Q4_0.gguf If everything works well, go to http://localhost:8000 and there you will see a neat web interface for chatting running.\nIf existing models are published on HuggingFace, we can run them directly from there without manually downloading them using the -hf parameter:\nllama-server --host localhost \\ --port 8000 \\ -hf unsloth/Qwen3-VL-8B-Instruct-GGUF:UD-Q4_K_XL Multimodal inference As mentioned before, the peculiarity of multimodal models (VL type) is the ability to process images. For this, specifying the mmproj file is essential:\nllama-server --host localhost --port 8000 \\ --model models/Qwen3VL/Qwen3-VL-8B-Instruct-UD-Q4_K_XL.gguf \\ --mmproj models/Qwen3VL/mmproj-F16.gguf \\ --ctx-size 4096 \\ --temp 0.7 \\ --flash-attn on \\ --jinja \\ --n-gpu-layers 99 \\ --top-k 20 \\ --top-p 0.8 \\ --min-p 0.0 \\ --presence-penalty 1.5 To know the meaning of the rest of the parameters, read the help for the llama-server command.\nUsage with Ollama If we want to use our GGUF files with Ollama, create a Modelfile with the following lines:\nFROM /path/to/file.gguf And then, just:\nollama create mymodel For more information, read the documentation on the Ollama website\nUpload GGUF files to our HF repository Finally, if we want to make these files available, we can upload them to our HuggingFace repository using the hf command.\nhf upload itzune/Latxa-Qwen3-VL-4B-GGUF HiTZ-Latxa-Qwen3-VL-4B-Instruct-q8_0.gguf ","wordCount":"639","inLanguage":"en","image":"https://xezpeleta.github.io/images/hf-ereduak-bihurtu-cover.png","datePublished":"2026-02-08T00:00:00Z","dateModified":"2026-02-08T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://xezpeleta.github.io/en/blog/hf-ereduak-bihurtu/"},"publisher":{"@type":"Organization","name":"Xabi Ezpeleta","logo":{"@type":"ImageObject","url":"https://xezpeleta.github.io/favicon.ico"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://xezpeleta.github.io/en/ accesskey=h title="Xabi Ezpeleta (Alt + H)">Xabi Ezpeleta</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://xezpeleta.github.io/ title=Euskara aria-label=Euskara>Eu</a></li></ul></div></div><ul id=menu><li><a href=https://xezpeleta.github.io/en/categories/ title=categories><span>categories</span></a></li><li><a href=https://xezpeleta.github.io/en/tags/ title=tags><span>tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Convert Models to GGUF Format</h1><div class=post-meta><span title='2026-02-08 00:00:00 +0000 UTC'>February 8, 2026</span>&nbsp;|&nbsp;<span>Translations:</span><ul class=i18n_list><li><a href=https://xezpeleta.github.io/blog/hf-ereduak-bihurtu/>Eu</a></li></ul></div></header><figure class=entry-cover><img loading=eager src=https://xezpeleta.github.io/images/hf-ereduak-bihurtu-cover.png alt="Convert HF models to GGUF format"><figcaption>Convert HF models to GGUF format</figcaption></figure><div class=post-content><p>This guide explains the steps to convert models published on HuggingFace to GGUF format.</p><p>The GGUF format offers the possibility to run models efficiently on our computers. For this, we can use tools like <a href=https://github.com/ggml-org/llama.cpp>Llama.cpp</a> or <a href=https://ollama.com/>Ollama</a>.</p><h2 id=download-llamacpp>Download Llama.cpp<a hidden class=anchor aria-hidden=true href=#download-llamacpp>#</a></h2><p>To convert to GGUF format locally on our machine, we first need to download <a href=https://github.com/ggml-org/llama.cpp>llama.cpp</a> to be able to use its conversion tools.</p><pre tabindex=0><code># Clone the llama.cpp repository
git clone https://github.com/ggml-org/llama.cpp.git
</code></pre><h2 id=prepare-the-environment>Prepare the environment<a hidden class=anchor aria-hidden=true href=#prepare-the-environment>#</a></h2><p>We will use <code>uv</code> to prepare the environment and install necessary dependencies. If you haven&rsquo;t installed it, <a href=https://docs.astral.sh/uv/getting-started/installation/>you will need to install it</a>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#75715e># Create a virtual environment</span>
</span></span><span style=display:flex><span>cd llama.cpp
</span></span><span style=display:flex><span>uv venv
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># If you encounter issues, try specifying the Python version</span>
</span></span><span style=display:flex><span><span style=color:#75715e># uv venv --python 3.11</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Activate the environment</span>
</span></span><span style=display:flex><span>source .venv/bin/activate
</span></span></code></pre></div><h2 id=install-dependencies>Install dependencies<a hidden class=anchor aria-hidden=true href=#install-dependencies>#</a></h2><p>In this case, we don&rsquo;t need to install the full <em>llama.cpp</em> to convert models; we just need the libraries to use the conversion tools:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-sh data-lang=sh><span style=display:flex><span>uv pip install -r requirements/requirements-convert_hf_to_gguf.txt
</span></span></code></pre></div><h2 id=convert-to-gguf-format>Convert to GGUF format<a hidden class=anchor aria-hidden=true href=#convert-to-gguf-format>#</a></h2><p>By passing the identifier of the model located on HuggingFace (<em>&lt;user_id>/&lt;repo_id></em>) and specifying the desired output type, it will create our gguf file:</p><p>In this case, we will convert the 4B version of the <code>HiTZ/Latxa-Qwen3-VL-4B-Instruct</code> model, with <code>f16</code> and <code>q8_0</code> quantizations.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#75715e># F16</span>
</span></span><span style=display:flex><span>uv run convert_hf_to_gguf.py HiTZ/Latxa-Qwen3-VL-4B-Instruct --remote --outtype f16
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Q8_0</span>
</span></span><span style=display:flex><span>uv run convert_hf_to_gguf.py HiTZ/Latxa-Qwen3-VL-4B-Instruct --remote --outtype q8_0
</span></span></code></pre></div><p>These commands will create the gguf files (e.g. <code>HiTZ-Latxa-Qwen3-VL-4B-Instruct-q8_0.gguf</code>).</p><h2 id=mmproj-image-processor-for-multimodal-models-vl><code>mmproj</code> image processor for multimodal models (VL)<a hidden class=anchor aria-hidden=true href=#mmproj-image-processor-for-multimodal-models-vl>#</a></h2><p>To be able to pass images to VL type multimodal models (e.g. Latxa VL models), we will also need the image encoder.
They are usually files named <code>mmproj</code>, which are often shared along with the model weights. We can download and use the original one (in this case, <a href=https://huggingface.co/Qwen/Qwen3-VL-4B-Instruct-GGUF/blob/main/mmproj-Qwen3VL-4B-Instruct-Q8_0.gguf>Qwen model&rsquo;s one</a>).</p><p>However, we can also create this file ourselves, using the same conversion script and passing the <code>--mmproj</code> parameter.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#75715e># Create mmproj file by adding `--mmproj`</span>
</span></span><span style=display:flex><span>uv run convert_hf_to_gguf.py HiTZ/Latxa-Qwen3-VL-2B-Instruct --remote --outfile mmproj-HiTZ-Latxa-Qwen3-VL-2B-Instruct-q8_0.gguf --outtype q8_0 --mmproj
</span></span></code></pre></div><h2 id=obtaining-other-quantizations>Obtaining other quantizations<a hidden class=anchor aria-hidden=true href=#obtaining-other-quantizations>#</a></h2><p>If you want to obtain other quantizations, you will need to follow the Llama.cpp installation steps and use the <code>llama-quantize</code> command.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#75715e># Quantize as Q4_K_M</span>
</span></span><span style=display:flex><span>./llama-quantize ggml-model-f16.gguf ggml-model-Q4_K_M.gguf Q4_K_M
</span></span></code></pre></div><p>For more information, read the <a href=https://github.com/ggml-org/llama.cpp/blob/master/tools/quantize/README.md>llama-quantize guide</a></p><h2 id=usage-with-llamacpp>Usage with <code>llama.cpp</code><a hidden class=anchor aria-hidden=true href=#usage-with-llamacpp>#</a></h2><p>Once our gguf files are created, we can run the models locally on our computer using <code>llama.cpp</code>.</p><p>For this, yes, <a href=https://github.com/ggml-org/llama.cpp/blob/master/docs/install.md>we will need to install</a> <em>llama.cpp</em>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#75715e># Run llama-cli with our model</span>
</span></span><span style=display:flex><span>llama-cli -m Orai-Kimu-9B-GGUF-Q4_0.gguf
</span></span></code></pre></div><p>This command will allow us to do quick tests right in the terminal.</p><p>I prefer, however, to run a server compatible with the OpenAI API, complete with a web interface!</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-sh data-lang=sh><span style=display:flex><span>llama-server <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span>    --host localhost <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span>    --port <span style=color:#ae81ff>8000</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span>    --model Gemma-2b-GGUF-Q4_0.gguf 
</span></span></code></pre></div><p>If everything works well, go to <code>http://localhost:8000</code> and there you will see a neat web interface for chatting running.</p><p>If existing models are published on <em>HuggingFace</em>, we can run them directly from there without manually downloading them using the <code>-hf</code> parameter:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-sh data-lang=sh><span style=display:flex><span>llama-server
</span></span><span style=display:flex><span>    --host localhost <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span>    --port <span style=color:#ae81ff>8000</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span>    -hf unsloth/Qwen3-VL-8B-Instruct-GGUF:UD-Q4_K_XL
</span></span></code></pre></div><h3 id=multimodal-inference>Multimodal inference<a hidden class=anchor aria-hidden=true href=#multimodal-inference>#</a></h3><p>As mentioned before, the peculiarity of multimodal models (VL type) is the ability to process images. For this, specifying the <code>mmproj</code> file is essential:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-sh data-lang=sh><span style=display:flex><span>llama-server --host localhost --port <span style=color:#ae81ff>8000</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span>      --model models/Qwen3VL/Qwen3-VL-8B-Instruct-UD-Q4_K_XL.gguf <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span>      --mmproj models/Qwen3VL/mmproj-F16.gguf <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span>      --ctx-size <span style=color:#ae81ff>4096</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span>      --temp 0.7 <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span>      --flash-attn on <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span>      --jinja <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span>      --n-gpu-layers <span style=color:#ae81ff>99</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span>      --top-k <span style=color:#ae81ff>20</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span>      --top-p 0.8 <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span>      --min-p 0.0 <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span>      --presence-penalty 1.5
</span></span></code></pre></div><p>To know the meaning of the rest of the parameters, read the help for the <code>llama-server</code> command.</p><h2 id=usage-with-ollama>Usage with Ollama<a hidden class=anchor aria-hidden=true href=#usage-with-ollama>#</a></h2><p>If we want to use our GGUF files with Ollama, create a <code>Modelfile</code> with the following lines:</p><pre tabindex=0><code>FROM /path/to/file.gguf
</code></pre><p>And then, just:</p><pre tabindex=0><code>ollama create mymodel
</code></pre><p>For more information, read the <a href=https://docs.ollama.com/import#importing-a-gguf-based-model-or-adapter>documentation on the Ollama website</a></p><h2 id=upload-gguf-files-to-our-hf-repository>Upload GGUF files to our HF repository<a hidden class=anchor aria-hidden=true href=#upload-gguf-files-to-our-hf-repository>#</a></h2><p>Finally, if we want to make these files available, we can upload them to our HuggingFace repository using the <code>hf</code> command.</p><pre tabindex=0><code>hf upload itzune/Latxa-Qwen3-VL-4B-GGUF HiTZ-Latxa-Qwen3-VL-4B-Instruct-q8_0.gguf
</code></pre></div><footer class=post-footer><ul class=post-tags><li><a href=https://xezpeleta.github.io/en/tags/llm/>Llm</a></li><li><a href=https://xezpeleta.github.io/en/tags/llama.cpp/>Llama.cpp</a></li><li><a href=https://xezpeleta.github.io/en/tags/gguf/>Gguf</a></li><li><a href=https://xezpeleta.github.io/en/tags/qwen/>Qwen</a></li></ul></footer></article></main><footer class=footer><span>Copyright © 2021, Xabier Ezpeleta. License <a href=https://creativecommons.org/licenses/by-sa/4.0/>CC BY-SA 4.0</a>.</span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>