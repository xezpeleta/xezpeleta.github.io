<!doctype html><html lang=eu dir=auto data-theme=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Ereduak GGUF formatura bihurtu | Xabi Ezpeleta</title><meta name=keywords content="llm,llama.cpp,gguf,qwen"><meta name=description content="Gida honetan HuggingFace webgunean argitaratuak dauden ereduak GGUF formatura bihurtzeko jarraibideak azaltzen dira.
GGUF formatu honek ereduak gure ordenagailuan modu eraginkorrean erabiltzeko aukera eskaintzen du. Horretarako, Llama.cpp edo Ollama bezalako tresnak erabili ditzakegu.
Llama.cpp deskargatu
Gure ordenagailuan bertan GGUF formatura bihurtzeko, llama.cpp
deskargatu beharko dugu lehenik eta behin, bertako bihurketa tresnak erabili ahal izateko.
# Klonatu llama.cpp biltegia
git clone https://github.com/ggml-org/llama.cpp.git
Ingurunea prestatu
Ingurunea prestatu eta beharrezko dependentziak instalatzeko, uv erabiliko dugu. Ez baduzu instalatu, instalatu beharko duzu."><meta name=author content><link rel=canonical href=https://xezpeleta.github.io/blog/hf-ereduak-bihurtu/><link crossorigin=anonymous href=/assets/css/stylesheet.da3211e5ef867bf2b75fd5a6515cfed7195c011e8ab735694e203810a827097b.css integrity="sha256-2jIR5e+Ge/K3X9WmUVz+1xlcAR6KtzVpTiA4EKgnCXs=" rel="preload stylesheet" as=style><link rel=icon href=https://xezpeleta.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://xezpeleta.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://xezpeleta.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://xezpeleta.github.io/apple-touch-icon.png><link rel=mask-icon href=https://xezpeleta.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=eu href=https://xezpeleta.github.io/blog/hf-ereduak-bihurtu/><link rel=alternate hreflang=en href=https://xezpeleta.github.io/en/blog/hf-ereduak-bihurtu/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-ZJRN8P7X6R"></script><script>var doNotTrack=!1,dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes";if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-ZJRN8P7X6R")}</script><meta property="og:url" content="https://xezpeleta.github.io/blog/hf-ereduak-bihurtu/"><meta property="og:site_name" content="Xabi Ezpeleta"><meta property="og:title" content="Ereduak GGUF formatura bihurtu"><meta property="og:description" content="Gida honetan HuggingFace webgunean argitaratuak dauden ereduak GGUF formatura bihurtzeko jarraibideak azaltzen dira.
GGUF formatu honek ereduak gure ordenagailuan modu eraginkorrean erabiltzeko aukera eskaintzen du. Horretarako, Llama.cpp edo Ollama bezalako tresnak erabili ditzakegu.
Llama.cpp deskargatu Gure ordenagailuan bertan GGUF formatura bihurtzeko, llama.cpp deskargatu beharko dugu lehenik eta behin, bertako bihurketa tresnak erabili ahal izateko.
# Klonatu llama.cpp biltegia git clone https://github.com/ggml-org/llama.cpp.git Ingurunea prestatu Ingurunea prestatu eta beharrezko dependentziak instalatzeko, uv erabiliko dugu. Ez baduzu instalatu, instalatu beharko duzu."><meta property="og:locale" content="eu"><meta property="og:type" content="article"><meta property="article:section" content="blog"><meta property="article:published_time" content="2026-02-08T00:00:00+00:00"><meta property="article:modified_time" content="2026-02-08T00:00:00+00:00"><meta property="article:tag" content="Llm"><meta property="article:tag" content="Llama.cpp"><meta property="article:tag" content="Gguf"><meta property="article:tag" content="Qwen"><meta property="og:image" content="https://xezpeleta.github.io/images/hf-ereduak-bihurtu-cover.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://xezpeleta.github.io/images/hf-ereduak-bihurtu-cover.png"><meta name=twitter:title content="Ereduak GGUF formatura bihurtu"><meta name=twitter:description content="Gida honetan HuggingFace webgunean argitaratuak dauden ereduak GGUF formatura bihurtzeko jarraibideak azaltzen dira.
GGUF formatu honek ereduak gure ordenagailuan modu eraginkorrean erabiltzeko aukera eskaintzen du. Horretarako, Llama.cpp edo Ollama bezalako tresnak erabili ditzakegu.
Llama.cpp deskargatu
Gure ordenagailuan bertan GGUF formatura bihurtzeko, llama.cpp
deskargatu beharko dugu lehenik eta behin, bertako bihurketa tresnak erabili ahal izateko.
# Klonatu llama.cpp biltegia
git clone https://github.com/ggml-org/llama.cpp.git
Ingurunea prestatu
Ingurunea prestatu eta beharrezko dependentziak instalatzeko, uv erabiliko dugu. Ez baduzu instalatu, instalatu beharko duzu."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://xezpeleta.github.io/blog/"},{"@type":"ListItem","position":2,"name":"Ereduak GGUF formatura bihurtu","item":"https://xezpeleta.github.io/blog/hf-ereduak-bihurtu/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Ereduak GGUF formatura bihurtu","name":"Ereduak GGUF formatura bihurtu","description":"Gida honetan HuggingFace webgunean argitaratuak dauden ereduak GGUF formatura bihurtzeko jarraibideak azaltzen dira.\nGGUF formatu honek ereduak gure ordenagailuan modu eraginkorrean erabiltzeko aukera eskaintzen du. Horretarako, Llama.cpp edo Ollama bezalako tresnak erabili ditzakegu.\nLlama.cpp deskargatu Gure ordenagailuan bertan GGUF formatura bihurtzeko, llama.cpp deskargatu beharko dugu lehenik eta behin, bertako bihurketa tresnak erabili ahal izateko.\n# Klonatu llama.cpp biltegia git clone https://github.com/ggml-org/llama.cpp.git Ingurunea prestatu Ingurunea prestatu eta beharrezko dependentziak instalatzeko, uv erabiliko dugu. Ez baduzu instalatu, instalatu beharko duzu.\n","keywords":["llm","llama.cpp","gguf","qwen"],"articleBody":"Gida honetan HuggingFace webgunean argitaratuak dauden ereduak GGUF formatura bihurtzeko jarraibideak azaltzen dira.\nGGUF formatu honek ereduak gure ordenagailuan modu eraginkorrean erabiltzeko aukera eskaintzen du. Horretarako, Llama.cpp edo Ollama bezalako tresnak erabili ditzakegu.\nLlama.cpp deskargatu Gure ordenagailuan bertan GGUF formatura bihurtzeko, llama.cpp deskargatu beharko dugu lehenik eta behin, bertako bihurketa tresnak erabili ahal izateko.\n# Klonatu llama.cpp biltegia git clone https://github.com/ggml-org/llama.cpp.git Ingurunea prestatu Ingurunea prestatu eta beharrezko dependentziak instalatzeko, uv erabiliko dugu. Ez baduzu instalatu, instalatu beharko duzu.\n# Sortu ingurune birtuala cd llama.cpp uv venv # Arazorik izanez gero, saiatu Python bertsioa zehazten # uv venv --python 3.11 # Aktibatu ingurunea source .venv/bin/activate Instalatu dependentziak Kasu honetan, ereduak bihurtzeko ez dugu behar llama.cpp osoa instalatzea; soilik bihurketa tresnak erabili ahal izateko liburutegiak beharko ditugu:\nuv pip install -r requirements/requirements-convert_hf_to_gguf.txt Bihurtu GGUF formatura HuggingFace webgunean dagoen ereduaren identifikatzailea (/) pasata eta nahi dugun irteera mota zehaztuta, gure gguf fitxategia sortuko digu:\nKasu honetan, HiTZ/Latxa-Qwen3-VL-4B-Instruct ereduaren 4B bertsioa bihurtuko dugu, f16 eta q8_0 kuantizazioekin.\n# F16 uv run convert_hf_to_gguf.py HiTZ/Latxa-Qwen3-VL-4B-Instruct --remote --outtype f16 # Q8_0 uv run convert_hf_to_gguf.py HiTZ/Latxa-Qwen3-VL-4B-Instruct --remote --outtype q8_0 Agindu hauek gguf fitxategiak sortuko dizkigu (adibidez HiTZ-Latxa-Qwen3-VL-4B-Instruct-q8_0.gguf).\nEredu multimodalentzako (VL) mmproj irudi prozesatzailea VL motako eredu multimodalei (adibidez Latxa VL ereduak) irudiak pasa ahal izateko, irudi kodifikatzailea ere beharko dugu. mmproj izeneko fitxategiak izan ohi dira, ereduaren pisuekin batera elkarbanatu ohi direnak. Jatorrizkoa deskargatu dezakegu eta erabili (kasu honetan, Qwen ereduarena).\nHala ere, fitxategi hau ere guk geuk sortu dezkegu, bihurketa scripta bera erabilita eta --mmproj parametroa pasata.\n# Sortu mmproj fitxategia `--mmproj` gehituta uv run convert_hf_to_gguf.py HiTZ/Latxa-Qwen3-VL-2B-Instruct --remote --outfile mmproj-HiTZ-Latxa-Qwen3-VL-2B-Instruct-q8_0.gguf --outtype q8_0 --mmproj Bestelako kuantizazioak lortzeko Bestelako kuantizazioak lortu nahi ezkero, Llama.cpp instalazio urratsak jarraitu beharko dituzu eta llama-quantize agindua erabili.\n# Kuantizatu Q4_K_M bezala ./llama-quantize ggml-model-f16.gguf ggml-model-Q4_K_M.gguf Q4_K_M Informazio gehiagorako, irakurri llama-quantize gida\nErabilera llama.cpp bidez Behin gure gguf fitxategiak sortuta, llama.cpp erabiliz ereduak exekutatu ditzakegu gure ordenagailuan bertan.\nHonetarako bai, llama.cpp instalatu beharko dugu.\n# Exekutatu llama-cli gure ereduarekin llama-cli -m Orai-Kimu-9B-GGUF-Q4_0.gguf Agindu honek terminalean bertan proba azkarrak egiteko aukera emango digu.\nNik nahiago, ordea, OpenAI APIarekin bateragarria den zerbitzaria exekutatu, web interfazea eta guzti!\nllama-server \\ --host localhost \\ --port 8000 \\ --model Gemma-2b-GGUF-Q4_0.gguf Dena ondo badabil, http://localhost:8000 helbidean sartu eta hor ikusiko duzu elkarrizketarako web interfaze txukuna martxan.\nEreduak HuggingFace-en argitaratuak badaude, zuzenean bertatik exekutatu ditzakegu eskuz deskargatu gabe -hf parametroa erabiliz:\nllama-server --host localhost \\ --port 8000 \\ -hf unsloth/Qwen3-VL-8B-Instruct-GGUF:UD-Q4_K_XL Eredu multimodalen inferentzia Lehen aipatu bezala, eredu multimodalek (VL motakoak) duten berezitasuna irudiak prozesatzeko gaitasuna da. Horretarako, mmproj fitxategia zehaztea ezinbestekoa da:\nllama-server --host localhost --port 8000 \\ --model models/Qwen3VL/Qwen3-VL-8B-Instruct-UD-Q4_K_XL.gguf \\ --mmproj models/Qwen3VL/mmproj-F16.gguf \\ --ctx-size 4096 \\ --temp 0.7 \\ --flash-attn on \\ --jinja \\ --n-gpu-layers 99 \\ --top-k 20 \\ --top-p 0.8 \\ --min-p 0.0 \\ --presence-penalty 1.5 Gainontzeko parametroen esanahia ezagutzeko, irakurri llama-server aginduaren laguntza.\nErabilera Ollama bidez Gure GGUF fitxategiak Ollama bidez erabili nahi baditugu, sortu Modelfile fitxategi bat ondorengo lerroekin:\nFROM /path/to/file.gguf Eta ondoren, soilik:\nollama create mymodel Informazio gehiagorako, irakurri Ollama webguneko dokumentazioa\nIgo GGUF fitxategiak gure HF biltegira Azkenik, fitxategi hauek eskuragarri utzi nahi baditugu, gure HuggingFace biltegira igo ditzakegu hf agindua erabiliz.\nhf upload itzune/Latxa-Qwen3-VL-4B-GGUF HiTZ-Latxa-Qwen3-VL-4B-Instruct-q8_0.gguf ","wordCount":"521","inLanguage":"eu","image":"https://xezpeleta.github.io/images/hf-ereduak-bihurtu-cover.png","datePublished":"2026-02-08T00:00:00Z","dateModified":"2026-02-08T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://xezpeleta.github.io/blog/hf-ereduak-bihurtu/"},"publisher":{"@type":"Organization","name":"Xabi Ezpeleta","logo":{"@type":"ImageObject","url":"https://xezpeleta.github.io/favicon.ico"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://xezpeleta.github.io/ accesskey=h title="Xabi Ezpeleta (Alt + H)">Xabi Ezpeleta</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://xezpeleta.github.io/en/ title=English aria-label=English>En</a></li></ul></div></div><ul id=menu><li><a href=https://xezpeleta.github.io/categories/ title=kategoriak><span>kategoriak</span></a></li><li><a href=https://xezpeleta.github.io/tags/ title=etiketak><span>etiketak</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Ereduak GGUF formatura bihurtu</h1><div class=post-meta><span title='2026-02-08 00:00:00 +0000 UTC'>2026(e)ko otsailaren 8(a)</span>&nbsp;|&nbsp;<span>Translations:</span><ul class=i18n_list><li><a href=https://xezpeleta.github.io/en/blog/hf-ereduak-bihurtu/>En</a></li></ul></div></header><figure class=entry-cover><img loading=eager src=https://xezpeleta.github.io/images/hf-ereduak-bihurtu-cover.png alt="HF ereduak GGUF formatura bihurtu"><figcaption>HF ereduak GGUF formatura bihurtu</figcaption></figure><div class=post-content><p>Gida honetan HuggingFace webgunean argitaratuak dauden ereduak GGUF formatura bihurtzeko jarraibideak azaltzen dira.</p><p>GGUF formatu honek ereduak gure ordenagailuan modu eraginkorrean erabiltzeko aukera eskaintzen du. Horretarako, <a href=https://github.com/ggml-org/llama.cpp>Llama.cpp</a> edo <a href=https://ollama.com/>Ollama</a> bezalako tresnak erabili ditzakegu.</p><h2 id=llamacpp-deskargatu>Llama.cpp deskargatu<a hidden class=anchor aria-hidden=true href=#llamacpp-deskargatu>#</a></h2><p>Gure ordenagailuan bertan GGUF formatura bihurtzeko, <a href=https://github.com/ggml-org/llama.cpp>llama.cpp</a>
deskargatu beharko dugu lehenik eta behin, bertako bihurketa tresnak erabili ahal izateko.</p><pre tabindex=0><code># Klonatu llama.cpp biltegia
git clone https://github.com/ggml-org/llama.cpp.git
</code></pre><h2 id=ingurunea-prestatu>Ingurunea prestatu<a hidden class=anchor aria-hidden=true href=#ingurunea-prestatu>#</a></h2><p>Ingurunea prestatu eta beharrezko dependentziak instalatzeko, <code>uv</code> erabiliko dugu. Ez baduzu instalatu, <a href=https://docs.astral.sh/uv/getting-started/installation/>instalatu beharko duzu</a>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#75715e># Sortu ingurune birtuala</span>
</span></span><span style=display:flex><span>cd llama.cpp
</span></span><span style=display:flex><span>uv venv
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Arazorik izanez gero, saiatu Python bertsioa zehazten</span>
</span></span><span style=display:flex><span><span style=color:#75715e># uv venv --python 3.11</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Aktibatu ingurunea</span>
</span></span><span style=display:flex><span>source .venv/bin/activate
</span></span></code></pre></div><h2 id=instalatu-dependentziak>Instalatu dependentziak<a hidden class=anchor aria-hidden=true href=#instalatu-dependentziak>#</a></h2><p>Kasu honetan, ereduak bihurtzeko ez dugu behar <em>llama.cpp</em> osoa instalatzea; soilik bihurketa tresnak erabili ahal izateko liburutegiak beharko ditugu:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-sh data-lang=sh><span style=display:flex><span>uv pip install -r requirements/requirements-convert_hf_to_gguf.txt
</span></span></code></pre></div><h2 id=bihurtu-gguf-formatura>Bihurtu GGUF formatura<a hidden class=anchor aria-hidden=true href=#bihurtu-gguf-formatura>#</a></h2><p>HuggingFace webgunean dagoen ereduaren identifikatzailea (<em>&lt;user_id>/&lt;repo_id></em>) pasata eta nahi dugun irteera mota zehaztuta,
gure gguf fitxategia sortuko digu:</p><p>Kasu honetan, <code>HiTZ/Latxa-Qwen3-VL-4B-Instruct</code> ereduaren 4B bertsioa bihurtuko dugu, <code>f16</code> eta <code>q8_0</code> kuantizazioekin.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#75715e># F16</span>
</span></span><span style=display:flex><span>uv run convert_hf_to_gguf.py HiTZ/Latxa-Qwen3-VL-4B-Instruct --remote --outtype f16
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Q8_0</span>
</span></span><span style=display:flex><span>uv run convert_hf_to_gguf.py HiTZ/Latxa-Qwen3-VL-4B-Instruct --remote --outtype q8_0
</span></span></code></pre></div><p>Agindu hauek gguf fitxategiak sortuko dizkigu (adibidez <code>HiTZ-Latxa-Qwen3-VL-4B-Instruct-q8_0.gguf</code>).</p><h2 id=eredu-multimodalentzako-vl-mmproj-irudi-prozesatzailea>Eredu multimodalentzako (VL) <code>mmproj</code> irudi prozesatzailea<a hidden class=anchor aria-hidden=true href=#eredu-multimodalentzako-vl-mmproj-irudi-prozesatzailea>#</a></h2><p>VL motako eredu multimodalei (adibidez Latxa VL ereduak) irudiak pasa ahal izateko, irudi kodifikatzailea ere beharko dugu.
<code>mmproj</code> izeneko fitxategiak izan ohi dira, ereduaren pisuekin batera elkarbanatu ohi direnak. Jatorrizkoa deskargatu dezakegu eta erabili (kasu honetan, <a href=https://huggingface.co/Qwen/Qwen3-VL-4B-Instruct-GGUF/blob/main/mmproj-Qwen3VL-4B-Instruct-Q8_0.gguf>Qwen ereduarena</a>).</p><p>Hala ere, fitxategi hau ere guk geuk sortu dezkegu, bihurketa scripta bera erabilita eta <code>--mmproj</code> parametroa pasata.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#75715e># Sortu mmproj fitxategia `--mmproj` gehituta</span>
</span></span><span style=display:flex><span>uv run convert_hf_to_gguf.py HiTZ/Latxa-Qwen3-VL-2B-Instruct --remote --outfile mmproj-HiTZ-Latxa-Qwen3-VL-2B-Instruct-q8_0.gguf --outtype q8_0 --mmproj
</span></span></code></pre></div><h2 id=bestelako-kuantizazioak-lortzeko>Bestelako kuantizazioak lortzeko<a hidden class=anchor aria-hidden=true href=#bestelako-kuantizazioak-lortzeko>#</a></h2><p>Bestelako kuantizazioak lortu nahi ezkero, Llama.cpp instalazio urratsak jarraitu beharko dituzu eta <code>llama-quantize</code> agindua erabili.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#75715e># Kuantizatu Q4_K_M bezala</span>
</span></span><span style=display:flex><span>./llama-quantize ggml-model-f16.gguf ggml-model-Q4_K_M.gguf Q4_K_M
</span></span></code></pre></div><p>Informazio gehiagorako, irakurri <a href=https://github.com/ggml-org/llama.cpp/blob/master/tools/quantize/README.md>llama-quantize gida</a></p><h2 id=erabilera-llamacpp-bidez>Erabilera <code>llama.cpp</code> bidez<a hidden class=anchor aria-hidden=true href=#erabilera-llamacpp-bidez>#</a></h2><p>Behin gure gguf fitxategiak sortuta, <code>llama.cpp</code> erabiliz ereduak exekutatu ditzakegu gure ordenagailuan bertan.</p><p>Honetarako bai, <em>llama.cpp</em> <a href=https://github.com/ggml-org/llama.cpp/blob/master/docs/install.md>instalatu beharko dugu</a>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#75715e># Exekutatu llama-cli gure ereduarekin</span>
</span></span><span style=display:flex><span>llama-cli -m Orai-Kimu-9B-GGUF-Q4_0.gguf
</span></span></code></pre></div><p>Agindu honek terminalean bertan proba azkarrak egiteko aukera emango digu.</p><p>Nik nahiago, ordea, OpenAI APIarekin bateragarria den zerbitzaria exekutatu, web interfazea eta guzti!</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-sh data-lang=sh><span style=display:flex><span>llama-server <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span>    --host localhost <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span>    --port <span style=color:#ae81ff>8000</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span>    --model Gemma-2b-GGUF-Q4_0.gguf 
</span></span></code></pre></div><p>Dena ondo badabil, <code>http://localhost:8000</code> helbidean sartu eta hor ikusiko duzu elkarrizketarako web interfaze txukuna martxan.</p><p>Ereduak <em>HuggingFace</em>-en argitaratuak badaude, zuzenean bertatik exekutatu ditzakegu eskuz deskargatu gabe <code>-hf</code> parametroa erabiliz:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-sh data-lang=sh><span style=display:flex><span>llama-server
</span></span><span style=display:flex><span>    --host localhost <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span>    --port <span style=color:#ae81ff>8000</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span>    -hf unsloth/Qwen3-VL-8B-Instruct-GGUF:UD-Q4_K_XL
</span></span></code></pre></div><h3 id=eredu-multimodalen-inferentzia>Eredu multimodalen inferentzia<a hidden class=anchor aria-hidden=true href=#eredu-multimodalen-inferentzia>#</a></h3><p>Lehen aipatu bezala, eredu multimodalek (VL motakoak) duten berezitasuna irudiak prozesatzeko gaitasuna da. Horretarako, <code>mmproj</code> fitxategia zehaztea ezinbestekoa da:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-sh data-lang=sh><span style=display:flex><span>llama-server --host localhost --port <span style=color:#ae81ff>8000</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span>      --model models/Qwen3VL/Qwen3-VL-8B-Instruct-UD-Q4_K_XL.gguf <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span>      --mmproj models/Qwen3VL/mmproj-F16.gguf <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span>      --ctx-size <span style=color:#ae81ff>4096</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span>      --temp 0.7 <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span>      --flash-attn on <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span>      --jinja <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span>      --n-gpu-layers <span style=color:#ae81ff>99</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span>      --top-k <span style=color:#ae81ff>20</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span>      --top-p 0.8 <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span>      --min-p 0.0 <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span>      --presence-penalty 1.5
</span></span></code></pre></div><p>Gainontzeko parametroen esanahia ezagutzeko, irakurri <code>llama-server</code> aginduaren laguntza.</p><h2 id=erabilera-ollama-bidez>Erabilera Ollama bidez<a hidden class=anchor aria-hidden=true href=#erabilera-ollama-bidez>#</a></h2><p>Gure GGUF fitxategiak Ollama bidez erabili nahi baditugu, sortu <code>Modelfile</code> fitxategi bat ondorengo lerroekin:</p><pre tabindex=0><code>FROM /path/to/file.gguf
</code></pre><p>Eta ondoren, soilik:</p><pre tabindex=0><code>ollama create mymodel
</code></pre><p>Informazio gehiagorako, irakurri <a href=https://docs.ollama.com/import#importing-a-gguf-based-model-or-adapter>Ollama webguneko dokumentazioa</a></p><h2 id=igo-gguf-fitxategiak-gure-hf-biltegira>Igo GGUF fitxategiak gure HF biltegira<a hidden class=anchor aria-hidden=true href=#igo-gguf-fitxategiak-gure-hf-biltegira>#</a></h2><p>Azkenik, fitxategi hauek eskuragarri utzi nahi baditugu, gure HuggingFace biltegira igo ditzakegu <code>hf</code> agindua erabiliz.</p><pre tabindex=0><code>hf upload itzune/Latxa-Qwen3-VL-4B-GGUF HiTZ-Latxa-Qwen3-VL-4B-Instruct-q8_0.gguf
</code></pre></div><footer class=post-footer><ul class=post-tags><li><a href=https://xezpeleta.github.io/tags/llm/>Llm</a></li><li><a href=https://xezpeleta.github.io/tags/llama.cpp/>Llama.cpp</a></li><li><a href=https://xezpeleta.github.io/tags/gguf/>Gguf</a></li><li><a href=https://xezpeleta.github.io/tags/qwen/>Qwen</a></li></ul></footer></article></main><footer class=footer><span>Copyright © 2021, Xabier Ezpeleta. License <a href=https://creativecommons.org/licenses/by-sa/4.0/>CC BY-SA 4.0</a>.</span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>